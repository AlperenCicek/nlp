{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I wanted you to see what real courage is, instead of getting the idea that courage is a man with a gun in his hand. It’s when you know you’re licked before you begin but you begin anyway and you see it through no matter what. You rarely win, but sometimes you do.'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "example_sent = \"I wanted you to see what real courage is, instead of getting the idea that courage is a man with a gun in his hand. It’s when you know you’re licked before you begin but you begin anyway and you see it through no matter what. You rarely win, but sometimes you do.\"\n",
    "example_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TOKENIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization In NLTK.Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization In NLTK.RegexpTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "word_tokens_without_punctuation = RegexpTokenizer(r'\\w+')\n",
    "word_tokens = word_tokens_without_punctuation.tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I\nwanted\nyou\nto\nsee\nwhat\nreal\ncourage\nis\ninstead\nof\ngetting\nthe\nidea\nthat\ncourage\nis\na\nman\nwith\na\ngun\nin\nhis\nhand\nIt\ns\nwhen\nyou\nknow\nyou\nre\nlicked\nbefore\nyou\nbegin\nbut\nyou\nbegin\nanyway\nand\nyou\nsee\nit\nthrough\nno\nmatter\nwhat\nYou\nrarely\nwin\nbut\nsometimes\nyou\ndo\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(word_tokens)):\n",
    "    print(word_tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['I',\n",
       " 'wanted',\n",
       " 'see',\n",
       " 'real',\n",
       " 'courage',\n",
       " ',',\n",
       " 'instead',\n",
       " 'getting',\n",
       " 'idea',\n",
       " 'courage',\n",
       " 'man',\n",
       " 'gun',\n",
       " 'hand',\n",
       " '.',\n",
       " 'It',\n",
       " '’',\n",
       " 'know',\n",
       " '’',\n",
       " 'licked',\n",
       " 'begin',\n",
       " 'begin',\n",
       " 'anyway',\n",
       " 'see',\n",
       " 'matter',\n",
       " '.',\n",
       " 'You',\n",
       " 'rarely',\n",
       " 'win',\n",
       " ',',\n",
       " 'sometimes',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "filtered_sw_sentence = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sw_sentence.append(w) \n",
    "filtered_sw_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LEMMATIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In lemmatization, the words are replaced by the root words or the words with similar context.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization In WordNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\Alperen\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizated_string_wordnet = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in filtered_sw_sentence: \n",
    "    lemmatizated_string_wordnet.append(lemmatizer.lemmatize(str(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization In Porter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizated_string_porter = []\n",
    "for i in range(len(filtered_sw_sentence)):\n",
    "    lemmatizated_string_porter.append(stemSentence(filtered_sw_sentence[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Porter vs WordNet In Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Porter vs WordNet\nI   <--->  I\nwant   <--->  wanted\nsee   <--->  see\nreal   <--->  real\ncourag   <--->  courage\n,   <--->  ,\ninstead   <--->  instead\nget   <--->  getting\nidea   <--->  idea\ncourag   <--->  courage\nman   <--->  man\ngun   <--->  gun\nhand   <--->  hand\n.   <--->  .\nIt   <--->  It\n’   <--->  ’\nknow   <--->  know\n’   <--->  ’\nlick   <--->  licked\nbegin   <--->  begin\nbegin   <--->  begin\nanyway   <--->  anyway\nsee   <--->  see\nmatter   <--->  matter\n.   <--->  .\nyou   <--->  You\nrare   <--->  rarely\nwin   <--->  win\n,   <--->  ,\nsometim   <--->  sometimes\n.   <--->  .\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter vs WordNet\")\n",
    "for i in range(len(lemmatizated_string_porter)):\n",
    "    print(lemmatizated_string_porter[i], \" <---> \", lemmatizated_string_wordnet[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}